# exp_name: tinyllama-hubertL-CV-10Hz-3earlyEOU-8random-shiftright-30pCrop-40pSil-sysprompt-Reload
# exp_name: tinyllama-hubertL-GS-10Hz
exp_name: tinyllamaR8-hubertL-GS-10Hz-EPD+late-Reload

model:
  # llm_name: "llama2-7b"
  llm_name: "tinyllama"
  # audio_enc_name: "wav2vec2"
  # audio_enc_name: "wav2vec2-bert"
  # audio_enc_name: "wav2vec2-bert-enconly"
  audio_enc_name: "hubert-large"
  # audio_enc_name: "whisper-large-v3"
  audio_stride: 5
  # init_type: "small"

model_load_path: "runs/tinyllama-hubertL-GS-10Hz-bs1/model*.safetensors"

dataset_name: "gigaspeech"
# dataset_name: "librispeech"
# dataset_name: "commonvoice"

audio_tokenizer_config:
  add_audio_tag_ratio: 0
  # prompt: "Transcribe speech to text: {audio}"
  prompt: "Transcribe speech to text and indicate whether user is done talking or they might continue with [END] or [...]: {audio}"
  late_eou_label: True
  # early_eou_labels: True
  early_eou_count: 3
  early_mid_count: 3
  crop_audio_prob: 0.6
  crop_audio_band: [0.4, 0.8]
  # crop_silence_prob: 0.4
  # system_prompt: "Note: After the end of a user turn, either 'END' or '...' is used to indicate whether the user is done or if his/her audio was cut-off before the end of their turn."

lr: 5e-5
# lr_warmup_ratio: 0.05
lr_warmup_steps: 2000
lr_scheduler_type: constant_with_warmup
# lr_scheduler_type: linear
# lr_scheduler_type: cosine_with_restarts
# lr_scheduler_kwargs:
#   num_cycles: 6
# optimizer_type: adamw_bnb_8bit
# weight_decay: 0.02
optimizer_type: adamw_torch

freezing_config:
  llm_lora_config:
    r: 8
    target_modules: [k_proj, q_proj]

  audio_enc_lora_config:
    r: 0
    target_modules: [k_proj, q_proj, linear_k, linear_q]

  freeze_text_embeds: True
  freeze_audio_embeds: True

num_workers: 4
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
eval_accumulation_steps: 1
# device: gpu
max_steps: 80000
logging_steps: 100
save_steps: 10000
eval_steps: 2000
bf16: True
# fp16: True
report_logs_to: ["tensorboard", "wandb"]
# workers: 32
